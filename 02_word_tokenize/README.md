## 中文分词项目

### 背景介绍
中文分词是一个 NLP 的基础任务，通常的做法有 基于词典分词算法 和 基于统计的机器学习算法。
基于词典的分词算法通常将分词任务看作是一个对于给定词典做文本序列的最大匹配任务，可以用动态规划等算法完成。优点是速度快，效果稳定，缺点是通常相对序列标注的方法效果稍逊一筹。
基于机器学习的分词算法通常将分词任务看作是一个序列标注任务，对一个序列的每一个字做 B(egin)I(nside)O(utside) 的多分类，可以用 (lstm+)crf 完成。优点是效果好，缺点是速度通常较基于词典的方法稍慢。


### 数据集介绍
数据集包括两部分：
1. dict.txt 为一个 50w 词的词表，每行三列，分别表示`词语 - 词频 - 词性`，可用于基于词典的分词方法。
2. 人民日报1998年01-06月语料库.zip 为人民日报分词语料，是一个经典的分词/pos数据集，通用中文分词器常使用该数据集训练得到 baseline 模型。既可用于 基于词典 和 基于序列标注的分词器训练，也可用于分词结果测试。

路径： 在 GPU 服务器上，位置是 `shannon-bootcamp/02_word_tokenize`

### 要求
用任一算法，实现`Tokenizer`类的`word_tokenize`方法，完成一个简易的分词器。可使用人民日报语料做测试。
